"""
Emergency cleanup script for stuck vLLM processes.
Run this if you get NCCL or process group warnings.
"""
import torch
import subprocess

def force_cleanup():
    """
    Emergency cleanup script for stuck vLLM processes.
    Run this if you get NCCL or process group warnings.
    # If you get stuck with NCCL warnings:

    uv run python -m vllm_srv.force_cleanup

    # Or manually:
    pkill -9 -f vllm
    python -c "import torch; torch.distributed.destroy_process_group() if torch.distributed.is_initialized() else None"

    """
    print("üßπ Emergency vLLM cleanup...")
    
    # 1. Destroy PyTorch distributed if initialized
    try:
        if torch.distributed.is_initialized():
            torch.distributed.destroy_process_group()
            print("‚úÖ Destroyed PyTorch process group")
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not destroy process group: {e}")
    
    # 2. Clear CUDA cache
    try:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            print("‚úÖ Cleared CUDA cache")
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not clear CUDA: {e}")
    
    # 3. Kill vLLM processes
    try:
        subprocess.run(["pkill", "-9", "-f", "vllm"], check=False)
        print("‚úÖ Killed vLLM processes")
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not kill processes: {e}")
    
    # 4. Kill Python processes using CUDA
    try:
        subprocess.run(["pkill", "-9", "-f", "python.*cuda"], check=False)
        print("‚úÖ Killed CUDA Python processes")
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not kill CUDA processes: {e}")
    
    print("‚úÖ Cleanup complete! Reboot recommended for cleanest state.")

if __name__ == "__main__":
    force_cleanup()